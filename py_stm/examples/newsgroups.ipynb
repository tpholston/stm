{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from py_stm.stm import StmModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18846 many documents in the newsgroups dataset\n",
      "Category breakdown (category : num_documents in category):\n",
      "0 : 799\n",
      "1 : 973\n",
      "2 : 985\n",
      "3 : 982\n",
      "4 : 963\n",
      "5 : 988\n",
      "6 : 975\n",
      "7 : 990\n",
      "8 : 996\n",
      "9 : 994\n",
      "10 : 999\n",
      "11 : 991\n",
      "12 : 984\n",
      "13 : 990\n",
      "14 : 987\n",
      "15 : 997\n",
      "16 : 910\n",
      "17 : 940\n",
      "18 : 775\n",
      "19 : 628\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(newsgroups.data)} many documents in the newsgroups dataset\")\n",
    "\n",
    "print(f\"Category breakdown (category : num_documents in category):\")\n",
    "for category, count in enumerate(np.bincount(newsgroups.target)):\n",
    "    print(f\"{category} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tylerholston/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Download the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = simple_preprocess(text, deacc=True, min_len=3)  # deacc=True removes punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split the text here to demonstrate model save/load functionality later\n",
    "train_text, test_text = train_test_split(newsgroups.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "processed_train_text = [preprocess_text(text) for text in train_text]\n",
    "processed_test_text = [preprocess_text(text) for text in test_text]\n",
    "\n",
    "# Create the training dictionary\n",
    "dictionary = Dictionary(processed_train_text)\n",
    "\n",
    "# Filter extremes (remove tokens that appear in less than 10 documents, or more than 50% of the documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "# Create the training corpus (bag of words representation)\n",
    "corpus_train = [dictionary.doc2bow(text) for text in processed_train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the STM model\n",
    "num_topics = 5  # Define the number of topics you want to extract\n",
    "stm = StmModel(corpus_train, num_topics=num_topics, id2word=dictionary, passes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"would\" + 0.007*\"space\" + 0.006*\"one\" + 0.004*\"also\" + 0.004*\"time\" + 0.004*\"like\" + 0.004*\"could\" + 0.004*\"may\" + 0.004*\"use\" + 0.003*\"much\"')\n",
      "(1, '0.012*\"edu\" + 0.009*\"dos\" + 0.009*\"windows\" + 0.007*\"use\" + 0.007*\"com\" + 0.007*\"file\" + 0.006*\"software\" + 0.006*\"image\" + 0.005*\"jpeg\" + 0.005*\"program\"')\n",
      "(2, '0.009*\"one\" + 0.009*\"would\" + 0.008*\"people\" + 0.006*\"think\" + 0.006*\"god\" + 0.005*\"like\" + 0.005*\"know\" + 0.004*\"time\" + 0.004*\"even\" + 0.004*\"well\"')\n",
      "(3, '0.016*\"max\" + 0.008*\"drive\" + 0.007*\"one\" + 0.006*\"would\" + 0.006*\"get\" + 0.006*\"like\" + 0.006*\"new\" + 0.005*\"use\" + 0.005*\"card\" + 0.005*\"car\"')\n",
      "(4, '0.007*\"government\" + 0.005*\"new\" + 0.005*\"armenian\" + 0.005*\"israel\" + 0.004*\"state\" + 0.004*\"people\" + 0.004*\"encryption\" + 0.004*\"key\" + 0.004*\"states\" + 0.004*\"university\"')\n"
     ]
    }
   ],
   "source": [
    "topics = stm.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the STM model\n",
    "stm.save(\"test_data/newsgroup_stm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for the first five test documents:\n",
      "Document 0:\n",
      "The runner can leave his base at any time.  If the ball is caught,\n",
      "he's got to tag up.  If it isn't ...\n",
      "[(2, 0.9815148)]\n",
      "\n",
      "Document 1:\n",
      "Well, it's not an FTP site, but I got an 800 number for Signetics BBS.\n",
      "\n",
      "The Signetics BBS contain so...\n",
      "[(1, 0.95122993), (3, 0.036857087)]\n",
      "\n",
      "Document 2:\n",
      "Hi,\n",
      "    I was reading through \"The Spaceflight Handbook\" and somewhere in\n",
      "there the author discusses...\n",
      "[(0, 0.97550154), (4, 0.010504319)]\n",
      "\n",
      "Document 3:\n",
      "I was a graduate student in the early 1980s, and we had a conference on \n",
      "Reaganomics where Jerry Jor...\n",
      "[(2, 0.49791273), (4, 0.49355567)]\n",
      "\n",
      "Document 4:\n",
      "FREE-ENERGY TECHNOLOGY\n",
      "                       by Robert E. McElwaine, Physicist\n",
      "          \n",
      "         ...\n",
      "[(0, 0.70560914), (1, 0.012896208), (2, 0.080270626), (3, 0.07121902), (4, 0.13000502)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the now saved STM\n",
    "stm = StmModel.load(\"test_data/newsgroup_stm\")\n",
    "\n",
    "# Classify the documents in the test set using the loaded LDA model\n",
    "corpus_test = [dictionary.doc2bow(text) for text in processed_test_text]\n",
    "doc_topics_test = [stm.get_document_topics(doc) for doc in corpus_test]\n",
    "\n",
    "# Display topics for the first five test documents (as an example)\n",
    "print(\"Topics for the first five test documents:\")\n",
    "for i in range(5):\n",
    "    print(f\"Document {i}:\")\n",
    "\n",
    "    # Lets print both the original text (first 100 characters) and the topic distribution\n",
    "    print(f\"{test_text[i].strip()[:100]}...\\n{doc_topics_test[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
