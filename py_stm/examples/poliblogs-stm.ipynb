{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of STM in use\n",
    "This is different than poliblogs.ipynb because we are demonstrating use of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrix\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from py_stm.stm import StmModel\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>docname</th>\n",
       "      <th>rating</th>\n",
       "      <th>day</th>\n",
       "      <th>blog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After a week of false statements, lies, and di...</td>\n",
       "      <td>at0800300_1.text</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I honestly don't know how either party's caucu...</td>\n",
       "      <td>at0800300_2.text</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While we stand in awe of the willingness of ou...</td>\n",
       "      <td>at0800300_3.text</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These pages recently said goodbye to global wa...</td>\n",
       "      <td>at0800300_4.text</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A US report shows how the enemy controlled the...</td>\n",
       "      <td>at0800300_5.text</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents           docname  \\\n",
       "0  After a week of false statements, lies, and di...  at0800300_1.text   \n",
       "1  I honestly don't know how either party's caucu...  at0800300_2.text   \n",
       "2  While we stand in awe of the willingness of ou...  at0800300_3.text   \n",
       "3  These pages recently said goodbye to global wa...  at0800300_4.text   \n",
       "4  A US report shows how the enemy controlled the...  at0800300_5.text   \n",
       "\n",
       "         rating  day blog  \n",
       "0  Conservative    3   at  \n",
       "1  Conservative    3   at  \n",
       "2  Conservative    3   at  \n",
       "3  Conservative    3   at  \n",
       "4  Conservative    3   at  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poliblogs = pd.read_csv(\"test_data/poliblogs2008.csv\", )\n",
    "poliblogs = poliblogs.loc[:, ~poliblogs.columns.str.contains('^Unnamed')]\n",
    "\n",
    "poliblogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13246 many documents in the poliblogs dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(poliblogs)} many documents in the poliblogs dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tylerholston/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Download the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, min_len):\n",
    "    tokens = simple_preprocess(text, deacc=True, min_len=min_len)  # deacc=True removes punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text and metadata for training\n",
    "train_text, test_text, train_metadata, test_metadata = train_test_split(\n",
    "    poliblogs.documents, poliblogs[['rating', 'day', 'blog']], test_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "processed_train_text = [preprocess_text(text, min_len=3) for text in train_text]\n",
    "processed_test_text = [preprocess_text(text, min_len=3) for text in test_text]\n",
    "\n",
    "# Create the training dictionary\n",
    "dictionary = Dictionary(processed_train_text)\n",
    "\n",
    "# Filter extremes (remove tokens that appear in less than 10 documents, or more than 50% of the documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "# Create the training corpus (bag of words representation)\n",
    "corpus_train = [dictionary.doc2bow(text) for text in processed_train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.46802029e-04, 2.93869088e-04, 7.97644667e-04, ...,\n",
       "       2.00780122e-05, 2.19032860e-05, 2.55538337e-05])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "corpus_train[190]\n",
    "doc_idx, word_idx, count = [], [], []\n",
    "\n",
    "for i, doc in enumerate(corpus_train):\n",
    "\tfor word, freq in doc:\n",
    "\t\tdoc_idx.append(i)\n",
    "\t\tword_idx.append(word)\n",
    "\t\tcount.append(freq)\n",
    "\n",
    "a = csr_matrix((count, (doc_idx, word_idx)))\n",
    "\n",
    "wprob = np.sum(a, axis=0)\n",
    "wprob = wprob / np.sum(wprob)    \n",
    "wprob = np.array(wprob)\n",
    "\n",
    "wprob.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>rating[T.Liberal]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11708</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Intercept  rating[T.Liberal]\n",
       "11708        1.0                1.0\n",
       "7650         1.0                0.0\n",
       "8599         1.0                0.0\n",
       "7788         1.0                0.0\n",
       "3456         1.0                1.0\n",
       "...          ...                ...\n",
       "11964        1.0                1.0\n",
       "5191         1.0                0.0\n",
       "5390         1.0                0.0\n",
       "860          1.0                0.0\n",
       "7270         1.0                0.0\n",
       "\n",
       "[2649 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A user could precompute the prevalence themselves like so\n",
    "from patsy import dmatrix\n",
    "\n",
    "# prevalence = dmatrix(\"~rating+cr(day, df=3)\", data=train_metadata, return_type='dataframe')\n",
    "prevalence = dmatrix(\"~rating\", data=train_metadata, return_type='dataframe')\n",
    "a = prevalence.astype(\"category\")\n",
    "prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerholston/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# Train the STM model\n",
    "num_topics = 5  # Define the number of topics you want to extract\n",
    "\n",
    "# PREVALENCE MODEL\n",
    "#stm = StmModel(corpus_train, num_topics=num_topics, id2word=dictionary, prevalence=prevalence, passes=10, random_state=420, chunksize=len(corpus_train)) # intended use 1. prevalence matrix precomputed\n",
    "\n",
    "# CONTENT MODEL\n",
    "stm = StmModel(corpus_train, num_topics=num_topics, id2word=dictionary, metadata=train_metadata, content=train_metadata.loc[:, \"rating\"], passes=10, random_state=420, chunksize=len(corpus_train)) # intended use 3. metadata dataframe and content formula\n",
    "\n",
    "# BOTH MODEL\n",
    "#stm = StmModel(corpus_train, num_topics=num_topics, id2word=dictionary, prevalence=prevalence, content=train_metadata.loc[:, \"rating\"], passes=10, random_state=420, chunksize=len(corpus_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Words:\n",
      "Topic 0: mccain, said, new, people, campaign, john, like, president, bush, time, even, think, get, iraq, also\n",
      "Topic 1: mccain, bush, said, people, new, iraq, campaign, like, president, time, even, john, get, think, also\n",
      "Topic 2: mccain, said, president, people, new, like, campaign, time, bush, even, think, american, get, also, john\n",
      "Topic 3: mccain, campaign, said, new, people, like, time, president, bush, even, get, also, john, think, barack\n",
      "Topic 4: mccain, said, new, people, like, campaign, time, president, even, bush, think, get, john, also, could\n",
      "\n",
      "Covariate Words:\n",
      "Group Liberal: mccain, like, people, new, even, said, campaign, time, two, barack, get, may, last, government, could\n",
      "Group Conservative: mccain, said, bush, president, campaign, people, new, john, iraq, think, like, know, right, time, today\n",
      "\n",
      "Topic-Covariate Interactions:\n",
      "Topic 0, Group Liberal: mccain, like, even, new, people, said, campaign, two, time, barack, get, may, last, government, could \n",
      "Topic 0, Group Conservative: mccain, said, john, bush, president, campaign, new, people, think, iraq, like, know, right, today, time \n",
      "Topic 1, Group Liberal: mccain, like, even, people, new, said, campaign, two, time, government, barack, get, may, last, could \n",
      "Topic 1, Group Conservative: bush, mccain, said, iraq, president, campaign, people, new, think, john, like, know, right, time, today \n",
      "Topic 2, Group Liberal: mccain, people, like, said, new, even, time, two, president, government, campaign, get, may, last, barack \n",
      "Topic 2, Group Conservative: mccain, said, president, people, new, bush, like, campaign, think, iraq, john, know, right, time, today \n",
      "Topic 3, Group Liberal: mccain, campaign, new, like, people, said, even, time, barack, two, get, last, may, also, could \n",
      "Topic 3, Group Conservative: mccain, said, campaign, new, people, bush, president, like, think, john, know, iraq, time, right, today \n",
      "Topic 4, Group Liberal: mccain, like, even, new, people, two, time, said, campaign, get, barack, could, may, last, also \n",
      "Topic 4, Group Conservative: mccain, said, bush, new, people, president, campaign, think, like, john, iraq, time, right, know, today \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['mccain', 'said', 'new', 'people', 'campaign', 'john', 'like',\n",
       "         'president', 'bush', 'time', 'even', 'think', 'get', 'iraq',\n",
       "         'also'],\n",
       "        ['mccain', 'bush', 'said', 'people', 'new', 'iraq', 'campaign',\n",
       "         'like', 'president', 'time', 'even', 'john', 'get', 'think',\n",
       "         'also'],\n",
       "        ['mccain', 'said', 'president', 'people', 'new', 'like',\n",
       "         'campaign', 'time', 'bush', 'even', 'think', 'american', 'get',\n",
       "         'also', 'john'],\n",
       "        ['mccain', 'campaign', 'said', 'new', 'people', 'like', 'time',\n",
       "         'president', 'bush', 'even', 'get', 'also', 'john', 'think',\n",
       "         'barack'],\n",
       "        ['mccain', 'said', 'new', 'people', 'like', 'campaign', 'time',\n",
       "         'president', 'even', 'bush', 'think', 'get', 'john', 'also',\n",
       "         'could']], dtype='<U10'),\n",
       " array([['mccain', 'like', 'people', 'new', 'even', 'said', 'campaign',\n",
       "         'time', 'two', 'barack', 'get', 'may', 'last', 'government',\n",
       "         'could'],\n",
       "        ['mccain', 'said', 'bush', 'president', 'campaign', 'people',\n",
       "         'new', 'john', 'iraq', 'think', 'like', 'know', 'right', 'time',\n",
       "         'today']], dtype='<U10'),\n",
       " array([['mccain', 'like', 'even', 'new', 'people', 'said', 'campaign',\n",
       "         'two', 'time', 'barack', 'get', 'may', 'last', 'government',\n",
       "         'could'],\n",
       "        ['mccain', 'like', 'even', 'people', 'new', 'said', 'campaign',\n",
       "         'two', 'time', 'government', 'barack', 'get', 'may', 'last',\n",
       "         'could'],\n",
       "        ['mccain', 'people', 'like', 'said', 'new', 'even', 'time', 'two',\n",
       "         'president', 'government', 'campaign', 'get', 'may', 'last',\n",
       "         'barack'],\n",
       "        ['mccain', 'campaign', 'new', 'like', 'people', 'said', 'even',\n",
       "         'time', 'barack', 'two', 'get', 'last', 'may', 'also', 'could'],\n",
       "        ['mccain', 'like', 'even', 'new', 'people', 'two', 'time', 'said',\n",
       "         'campaign', 'get', 'barack', 'could', 'may', 'last', 'also'],\n",
       "        ['mccain', 'said', 'john', 'bush', 'president', 'campaign', 'new',\n",
       "         'people', 'think', 'iraq', 'like', 'know', 'right', 'today',\n",
       "         'time'],\n",
       "        ['bush', 'mccain', 'said', 'iraq', 'president', 'campaign',\n",
       "         'people', 'new', 'think', 'john', 'like', 'know', 'right',\n",
       "         'time', 'today'],\n",
       "        ['mccain', 'said', 'president', 'people', 'new', 'bush', 'like',\n",
       "         'campaign', 'think', 'iraq', 'john', 'know', 'right', 'time',\n",
       "         'today'],\n",
       "        ['mccain', 'said', 'campaign', 'new', 'people', 'bush',\n",
       "         'president', 'like', 'think', 'john', 'know', 'iraq', 'time',\n",
       "         'right', 'today'],\n",
       "        ['mccain', 'said', 'bush', 'new', 'people', 'president',\n",
       "         'campaign', 'think', 'like', 'john', 'iraq', 'time', 'right',\n",
       "         'know', 'today']], dtype='<U10'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm.label_topics(range(0,5), n=15, print_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "1925    Stephen Hayes of the Weekly Standard has the s...\n",
      "1600    The Democratic Party has announced that Barack...\n",
      "706     As a followup to Andrew Walden's excellent pie...\n",
      "757     No, it's not a misprint. Officials with the ZA...\n",
      "1680    This article  (in the New York Times, no less)...\n",
      "2413    My jaw hit the floor when I heard Biden say th...\n",
      "Name: documents, dtype: object\n",
      "\n",
      "\n",
      "Topic: 2\n",
      "1811    On July 12th of this year, just two and a half...\n",
      "1970    Little movement has been made by Russia to tak...\n",
      "2193    Barack Obama apparently wishes his two autobio...\n",
      "333     The ultraliberal CBC reports a truth that is m...\n",
      "1711    I just learned of the following letter from Do...\n",
      "176     Bill and Hillary Clinton are aggressive politi...\n",
      "Name: documents, dtype: object\n",
      "\n",
      "\n",
      "Topic: 3\n",
      "1580    You stay classy, Bill Jeff:Bill Clinton is spe...\n",
      "1487    You have to wonder what will be going through ...\n",
      "2410    ACORN, responsible for so much vote fraud in W...\n",
      "1737    I was interested to see that I and many reader...\n",
      "61      Senator John Kerry will endorse Barack Obama s...\n",
      "752     The race for the Democratic presidential nomin...\n",
      "Name: documents, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = [1, 2, 3] # or range(2, 5)\n",
    "for topic, docs in zip(topics, stm.find_thoughts(topics, n=6)):\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(poliblogs.iloc[docs]['documents'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm.printTopTopics(\"topicCorr.png\", topics=np.array([0, 1, 2, 3, 4]), bbox=(0,0,500,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_accommodations = pyLDAvis.gensim_models.prepare(stm, corpus_train, stm.id2word, mds=\"mmds\")\n",
    "# vis_accommodations.save('visualizations/poliblogs.html') Saving doesn't work right now\n",
    "vis_accommodations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
